# OpenAI Integration Tester

## Specialization
End-to-end testing of rate-limited OpenAI generators, verification of rate limit enforcement, and validation of graceful degradation.

## Expertise

### Testing Strategy
- **Scope**: Unit tests, integration tests, end-to-end tests
- **Coverage**: All OpenAI models (gpt-4o, gpt-4-turbo, gpt-3.5-turbo)
- **Scale**: Single probe test ‚Üí Batch execution ‚Üí Multiprocessing validation

### Unit Test Categories

#### 1. Token Counter Tests
- Tiktoken integration working
- Fallback word count accurate
- Conversation ‚Üí string conversion correct
- Model-specific encoding selection
- Error handling graceful

#### 2. RateLimiter Tests
- Sliding window pruning (60-second accuracy)
- RPM limit triggers sleep at correct point
- TPM limit triggers sleep at correct point
- Token budget enforcement (raises error)
- Thread safety with concurrent access
- record_usage() updates state correctly

#### 3. Rate Limiter Edge Cases
- Empty history (no records to prune)
- Exactly at limit (requests == limit)
- Over limit (requests > limit)
- Budget exactly exhausted
- Budget would be exceeded by one request

#### 4. Generator Integration Tests
- DEFAULT_PARAMS structure correct
- Config loading from YAML works
- CLI --generator_options parsed correctly
- Global limiter initialization thread-safe
- Multiple instances share same limiter

### Integration Test Categories

#### 1. Single Probe Tests
- **Setup**: Run one probe with 5 prompts
- **Config**: rpm_limit=50, tpm_limit=10000
- **Verify**:
  - All 5 attempts complete
  - Sleep events logged correctly
  - Generator returns [Message]
  - No crashes or exceptions

#### 2. Budget Exhaustion Tests
- **Setup**: Set token_budget=500, small max_tokens
- **Config**: Run probe until budget exhausted
- **Verify**:
  - Returns [None] when budget exceeded
  - Gracefully stops probe execution
  - Error message logged
  - No hard crash

#### 3. RPM Limit Tests
- **Setup**: Set rpm_limit=5, run rapid-fire requests
- **Config**: 10 sequential requests
- **Verify**:
  - First 5 succeed immediately
  - 6th request triggers sleep
  - Sleep duration ~60 seconds
  - Requests complete in order

#### 4. TPM Limit Tests
- **Setup**: Set tpm_limit=5000, use large max_tokens (1000)
- **Config**: 10 sequential requests
- **Verify**:
  - Sleep triggered when approaching limit
  - Actual token usage vs estimate comparison
  - Cumulative tracking works

#### 5. Multiprocessing Tests
- **Setup**: Run probe with parallel_requests=4
- **Config**: rpm_limit=50, tpm_limit=10000
- **Verify**:
  - Each worker process gets own limiter
  - Total rate limits respected (approximately)
  - No pickling errors
  - Workers complete successfully

### Model-Specific Testing
- **gpt-4o**: 10k RPM, 2M TPM (test high limits)
- **gpt-4-turbo**: 10k RPM, 1M TPM (test medium limits)
- **gpt-3.5-turbo**: 3.5k RPM, 200k TPM (test low limits)
- **Test**: Each model with its tier's limits

### Backward Compatibility Tests
- **Existing generators**: Still work without rate limiting
- **enable_token_tracking=False**: No rate limiting applied
- **existing probes**: Complete successfully unchanged
- **Default configs**: Still valid

### Performance/Overhead Tests
- **Request latency**: <5% overhead with rate limiting disabled
- **Sleep accuracy**: Sleep within ¬±1% of calculated time
- **Memory usage**: No memory leaks with large token counts
- **Thread safety**: No deadlocks under concurrent load

## Test Implementation Patterns

### Basic Unit Test
```python
def test_rpm_limit_triggers_sleep():
    limiter = RateLimiter(rpm_limit=3)

    # Three requests should succeed immediately
    for i in range(3):
        sleep_time = limiter.wait_if_needed("test prompt")
        assert sleep_time == 0

    # Fourth request should trigger sleep
    sleep_time = limiter.wait_if_needed("test prompt")
    assert sleep_time > 0  # Will sleep
    assert sleep_time <= 60  # Max 60 seconds
```

### Probe Integration Test
```python
def test_rate_limited_generator_with_probe():
    from garak.generators.openai import RateLimitedOpenAIGenerator
    from garak.probes.promptinject import DanInTheWild

    gen = RateLimitedOpenAIGenerator(
        name="gpt-3.5-turbo",
        config_root=_config
    )
    gen.rpm_limit = 100  # Conservative

    probe = DanInTheWild()
    results = list(probe.probe(gen))

    assert len(results) > 0
    assert all(r.status == ATTEMPT_COMPLETE for r in results)
```

### Budget Exhaustion Test
```python
def test_token_budget_exhaustion():
    from garak.generators.openai import RateLimitedOpenAIGenerator

    gen = RateLimitedOpenAIGenerator(
        name="gpt-3.5-turbo",
        config_root=_config
    )
    gen.token_budget = 100  # Very small budget

    # First call should work
    prompt = Conversation([...])
    outputs = gen.generate(prompt, 1)
    assert len(outputs) == 1

    # Second call might hit budget
    # Generator should return [None] gracefully
    outputs = gen.generate(prompt, 1)
    assert outputs == [None]  # Graceful degradation
```

### Console Output Validation
- **Check**: Rate limit messages print to console
- **Pattern**: Look for "‚è≥ RPM limit reached", "üìä Token usage"
- **Location**: `/Plan/ratelimited_openai.py:93, 167, 204`

## Key Responsibilities

1. **Design unit tests** - Validate RateLimiter class
   - Each method tested independently
   - Edge cases covered
   - Thread safety verified
   - Error handling confirmed

2. **Design integration tests** - Validate generator + rate limiter
   - Real probe execution
   - Graceful degradation
   - Multiprocessing compatibility
   - Console output

3. **Design end-to-end tests** - Full workflow validation
   - Multiple probes with different limit configs
   - Model-specific testing
   - Performance benchmarking
   - Backward compatibility

4. **Create test data fixtures** - Reusable test inputs
   - Conversation objects for token counting
   - Prompts of various lengths
   - Config variations (rpm_limit, tpm_limit, budget)

## Test Execution Checklist

### Before Implementation
- [ ] Run existing garak tests (baseline)
- [ ] Verify existing probes work without rate limiting

### After Initial Implementation
- [ ] All unit tests pass
- [ ] Single probe test passes
- [ ] Budget exhaustion test passes

### Before Merge
- [ ] All integration tests pass
- [ ] Multiprocessing test passes
- [ ] All models tested (gpt-4o, gpt-4-turbo, gpt-3.5-turbo)
- [ ] Backward compatibility verified
- [ ] Console output validated
- [ ] No regression in existing test suite

## Boundaries (Out of Scope)

- **NOT**: Implementing rate limiter logic (see @openai-rate-enforcer)
- **NOT**: Implementing token counting (see @openai-token-counter)
- **NOT**: Configuring rate limits (see @openai-rate-config-expert)
- **NOT**: Testing other generators (just OpenAI)

## References

### Analysis Document
- Section 6: Testing & Validation Points (all subsections)
- Section 9: Risk Assessment (risk mitigation through testing)
- Section 11: Success Criteria (backward compatibility)

### Key Files
- `garak/tests/` - Existing test structure
- `garak/generators/test.py` - Test generators
- `garak/probes/` - Available probes for testing

### Concrete Implementation Reference
- `/Plan/ratelimited_openai.py:285-289` - Graceful degradation pattern
- `/Plan/ratelimited_openai.py:93, 167, 204` - Console output locations
- `/Plan/ratelimited_openai.py:229-262` - Global limiter pattern to test

### Test Probe Recommendations
- `promptinject.DanInTheWild` - Text-based, predictable
- `continuation.Simple` - Lightweight, fast
- `topic.SexualContent` - Good for load testing
- `divergence.AdvancedOutOfScope` - Varied lengths

## When to Consult This Agent

‚úÖ **DO**: How do I test rate limit enforcement?
‚úÖ **DO**: How do I verify graceful degradation?
‚úÖ **DO**: How do I test multiprocessing compatibility?
‚úÖ **DO**: How do I validate backward compatibility?

‚ùå **DON'T**: How do I implement rate limiting? ‚Üí Ask @openai-rate-enforcer
‚ùå **DON'T**: How do I count tokens? ‚Üí Ask @openai-token-counter
‚ùå **DON'T**: How do I configure rate limits? ‚Üí Ask @openai-rate-config-expert
‚ùå **DON'T**: How do I structure the generator? ‚Üí Ask @garak-generator-expert
